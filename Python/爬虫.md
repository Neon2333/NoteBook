# 1. http和https

* 常用请求头

  User-agent：请求载体身份标识

  Connection：请求完毕后是否断开连接

* 常用响应头

  Content-Type：服务器响应客户端的数据类型

* https：安全的http

  传输数据进行加密。

  证书加密

* 加密方式：

  对称加密

  非对称加密

  证书加密

# 2. 安装requests

`pip install requests`

# 3. 爬取html和json

```python
#设定url
url='https://www.baidu.com'   
#UA反扒，设定User-Agent
headers_list={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'}
#发get请求
response = requests.get(url,headers=headers_list)
#从抓包工具看响应的Content-type是html。把抓到的结果保存为html。
with open('nogi.html','w',encoding='utf-8') as fp:
    fp.write(response.text)
print('get success')
```

```python
import json

import requests

url='https://fanyi.baidu.com/sug'   

kw = input('enter keyword:')
data_list={'kw': kw}
headers_list={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'}

response = requests.post(url=url,data=data_list,headers=headers_list)
print('status code:{status_code}'.format(status_code=response.status_code))
#至于确定响应结果是json，才能用.json()否则报错。
res_dict = response.json()	

with open(kw+'.json','w',encoding='utf-8') as fp:
    json.dump(res_dict,fp=fp,ensure_ascii=False)
print('success..')
```

* 获取URL

  ![image-20240308232230630](https://raw.githubusercontent.com/Neon2333/ImageHost/main/image-20240308232230630.png?token=ANG32YAVM3PYKLJZ57XZNRLF5MXXI)

* 发出请求

  ```python
  headers_list={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'}
  ```

* 获取响应

  文本：.text

  二进制：.content

  json：json()

  ![image-20240308233718549](https://raw.githubusercontent.com/Neon2333/ImageHost/main/image-20240308233718549.png?token=ANG32YCEFWAS3CCY4IZAEGDF5MYUC)

* 数据解析

* 数据持久化

# 4. 数据解析

正则、bs4、xpath三种方式。

### 1）xpath

xpath解析：最常用且最便捷高效的一种解析方式。通用性。 

Chrome下F12，点击页面元素右键copy->xpath。

xpath解析原理： 

* 实例化一个etree的对象，且需要将被解析的页面源码数据加载到该对象中。 

* 调用etree对象中的xpath方法结合着xpath表达式实现标签的定位和内容的捕获。
  
* 环境的安装： pip install lxml

* 如何实例化etree对象

* 将本地的html文档中的源码数据加载到etree对象中：

  ```python
  from lxml import etree
  
  html=etree.parse('./.html,etree.HTMLParset()')
  ```

* 可以将从互联网上获取的源码数据加载到该对象中：

  ```python
  html=etree.HTML(html_text)
  ```

* xpath('xpath表达式')

  /：表示从根节点开始定位。一个层级。

  //：表示多个层级（跨层级）

  ./：表示当前层级

  ```python
  r=html.xpath('/html/body/div')
  r=html.xpath('/html//div')
  r=html.xpath('//div')
  ```

  **属性定位：**

  ```python
  r=html.xpath('//div[@class='属性值']')		#获取指定属性值的div
  ```

  **索引定位：**

  索引从1开始

  ```python
  r=html.xpath('//div[@class='属性值']/p[3]')	#获取指定属性值的div标签下的第三个p标签
  ```

  **取文本：**

  /text()：获取标签直系下的文本

  //text()：获取标签下所有文本

  ```python
  r=html.xpath('//div[@class='属性值']/p[3]/a/text()')[0]	#取标签div//p/a中文本
  r=html.xpath('//div[@class='属性值']/p[3]//text()')[0]		#取div//p下所有文本
  ```

  **取属性值：**

  /@属性名

  ```python
  r=html.xpath('//div[@class='属性值']/img/@src') #获取img标签的src属性值
  ```

### 2）获取图片

获取html，xpath定位到图片url。

* 通用处理中文乱码方式：

  ```python
  img_name = img_name.encode('iso-8859-1').decode('gbk')
  ```

  ```python
  # requests.get获取html后。
  #数据解析：获取src和alt属性
  tree = etree.HTML(html_text)
  li_list=tree.xpath('//div[@class="slist"]/ul/li')
  for li in li_list:
      img_src='http://pic.net.com' + li.xpath('./a/img/@src')[0]
      img_name=li.xpath('./a/img/@alt')[0]+'.jpg'
      img_name = img_name.encode('iso-8859-1').decode('gbk')	#处理乱码
     	#请求图片
      img_data=requests.get(url=img_src,headers=header_list).content
      with open('./'+img_name,'wb') as fp:
          fp.write(img_data)
  ```

# 5. 登录

## 1）验证码识别

反爬机制：验证码。

识别验证码模拟登录。

使用验证码识别库。

* 将验证码图片下载到本地
* 调用API识别图片获取字符串

## 2）模拟登陆

目的：爬取该网站的**指定用户的信息**

点击登录后，会发出POST请求，携带：用户名、密码、验证码

验证码：每次请求都会变化

```python
#流程
#通过API识别验证码
#将验证码、用户名、密码通过POST发送
#响应数据持久化

result_code=getCodeText('code.jpg')

#抓包获取下列信息（抓login数据包）
login_url=''
data={
    'email':'',
    'icode':result_code,
}

#发请求
response = requests.post(url=login_url,headers=header_list,data=data)
print('status code:{status_code}'.format(status_code=status_code))
with open('./login.html','w',encoding='utf-8') as fp:
    fp.write(login_text)
```

cookie：模拟登录POST请求后，客户端发送set-cookie后，服务器生成的。

**session对象：使用session对象发送POST请求时，如果产生了cookie，cookie会被存储/携带在session对象中。再使用该session对象进行get请求。**

```python
#get请求个人主页
#发起第二次请求时，需要cookie让服务器知道你已经登陆了。请求时要携带cookie给服务器。
#修改上面的POST代码
```

```python
response = session.post(url=login_url,headers=header_list,data=data)

profile_text=session.get(url=profile_url,headers=header_list).text
print(profile_text)
```

# 6.代理

反爬机制：网站检测到某IP在单位内访问过于频繁，会**封IP**。

代理服务器：本机通过请求发送给代理服务器，代理服务器发送请求给服务器，服务器把响应给代理服务器，代理服务器返回给本机。

隐藏本机IP。突破本机IP的访问限制。

代理ip类型：http/https。只能用在相同协议的url上。

代理ip的匿名度：透明-服务器知道使用了代理，也知道本机ip。匿名-知道使用了代理，不知道本机ip。高匿-服务器不知道使用了代理，也不知道本机ip。

```python
response=requesets.get(url=url,headers=header,proxies={'https':'代理ip'})
```

# 7. 利用线程池爬取

使用线程池，处理耗时的操作。

```python
import requests

#数据获取
def get_content(url):
    print('正在爬取:{url}'.format(url=url))
    response=requests.get(url=url,headers=headers)
    if response.status_code== '200':
        return response.content
#数据解析
def parse_content(content):
    #这里要根据实际需要什么内容来写
    
for url in urls:
    content=get_content(url)	#会阻塞在这里，需要网络数据响应
    parse_content(content)
```

```python
import requests
from multiprocessing.dummy import Pool
import re

url = 'https://www.pearvideo.com/category_5'
page_text=requests.get(url=url,headers=header).text

tree = etree.HTML(page_text)
li_list = tree.xpath('//ul[@id="listvideoListUl"]/li')
urls=[]	#存储视频url和name
for li in li_list:
    detail_url='https:www.pearvideo.com/'+li.xpath('./div/a/@href')[0]
    name=li.xpath('./div/a/div[2]/text()')[0]+'.mp4'
    #对视频详情页url发请求
    detail_page_text=requests.get(url=detail_url,headers=header).text
    #从视频详情页解析出视频url
    ex='srcUrl="(.*?)",vdoUrl'	#这个url在js代码里，xpath无法解析，使用正则解析
    video_url=re.findall(ex,deatil_page_text)[0]
    dic={
        'name':name,
        'url':video_url
    }
    urls.append(dic)
    
#对视频url请求函数
def get_video_data(dic):
    url=dic['url']
    print("{}正在下载：...".format(dic['name']))
    video_data=requests.get(url=url,headers=header).content
    with open('./'+dic['name'],'wb') as fp:
        fp.write(video_data)
        print('{}下载完成...'.format(dic['name']))
 
#urls中的视频url进行多线程请求
pool=Pool(4)	#线程池4个线程
pool.map(get_video_data,urls)
pool.join()	#主线程等待线程池内子线程全部执行完
pool.close()	#关闭线程池	
```

# 8. 单线程+异步协程





